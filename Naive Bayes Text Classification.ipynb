{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6: Naive Bayes Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classifiction\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    "The news are from each of the following 20 newsgroups.\n",
    "\n",
    "    alt.atheism\n",
    "    comp.graphics\n",
    "    comp.os.ms-windows.misc\n",
    "    comp.sys.ibm.pc.hardware\n",
    "    comp.sys.mac.hardware\n",
    "    comp.windows.x\n",
    "    misc.forsale\n",
    "    rec.autos\n",
    "    rec.motorcycles\n",
    "    rec.sport.baseball\n",
    "    rec.sport.hockey\n",
    "    sci.crypt\n",
    "    sci.electronics\n",
    "    sci.med\n",
    "    sci.space\n",
    "    soc.religion.christian\n",
    "    talk.politics.guns\n",
    "    talk.politics.mideast\n",
    "    talk.politics.misc\n",
    "    talk.religion.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 20 news group dataset from scikit learn datasets\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= fetch_20newsgroups()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sklearn.datasets.fetch_20newsgroups function is a data fetching / caching\n",
    "#functions that downloads the data archive from the original 20 newsgroups \n",
    "#website, extracts the archive contents in the ~/scikit_learn_data/20news_home \n",
    "#folder and calls the sklearn.datasets.load_files on either the training or \n",
    "#testing set folder.\n",
    "\n",
    "#load 20 news group train subset - You need Internet for it/Or place the data file in Anaconda directory\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "#load 20 news group test subset\n",
    "newsgroups_test = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all target labels - dataset.target_names\n",
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a list of categories 'alt.atheism', 'comp.graphics','sci.space'\n",
    "catlist = ['alt.atheism','comp.graphics','sci.space']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load 20 news group train subset with three categories 'alt.atheism', \n",
    "#'comp.graphics','sci.space' by passing the list to fetch_20newsgroups \n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=catlist, shuffle=True)\n",
    "\n",
    "#load 20 news group test subset with three categories 'alt.atheism', \n",
    "#'comp.graphics','sci.space' by passing the list to fetch_20newsgroups \n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=catlist, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "# print new training set target names(labels)\n",
    "print(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = newsgroups_train.data\n",
    "y = newsgroups_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1657, 1657)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print shape of targets\n",
    "#X = pd.DataFrame(X)\n",
    "#y = pd.DataFrame(y)\n",
    "#X.shape ,y.shape\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\RGUKT\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\sci.space\\\\60869'\n",
      " 'C:\\\\Users\\\\RGUKT\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.graphics\\\\38633'\n",
      " 'C:\\\\Users\\\\RGUKT\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\alt.atheism\\\\53534'\n",
      " ...\n",
      " 'C:\\\\Users\\\\RGUKT\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\sci.space\\\\60915'\n",
      " 'C:\\\\Users\\\\RGUKT\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\sci.space\\\\60176'\n",
      " 'C:\\\\Users\\\\RGUKT\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\sci.space\\\\60929']\n"
     ]
    }
   ],
   "source": [
    "#print training set filenames - dataset.filenames\n",
    "print(newsgroups_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dietz@cs.rochester.edu (Paul Dietz)\n",
      "Subject: Commercial mining activities on the moon\n",
      "Organization: University of Rochester\n",
      "Lines: 38\n",
      "\n",
      "In article <1993Apr20.152819.28186@ke4zv.uucp> gary@ke4zv.UUCP (Gary Coffman) writes:\n",
      "\n",
      " > be the site of major commercial activity. As far as we know it has no\n",
      " > materials we can't get cheaper right here on Earth or from asteroids\n",
      " > and comets, aside from the semi-mythic He3 that *might* be useful in low\n",
      " > grade fusion reactors.\n",
      "\n",
      "I don't know what a \"low grade\" fusion reactor is, but the major\n",
      "problem with 3He (aside from the difficulty in making any fusion\n",
      "reactor work) is that its concentration in lunar regolith is just so\n",
      "small -- on the order of 5 ppb or so, on average (more in some\n",
      "fractions, but still very small).  Massive amounts of regolith would\n",
      "have to be processed.\n",
      "\n",
      "This thread reminds me of Wingo's claims some time ago about the moon\n",
      "as a source of titanium for use on earth.  As I recall, Wingo wasn't\n",
      "content with being assured that titanium (at .5% in the Earth's crust,\n",
      "average) would not run out, and touted lunar mines, even though the\n",
      "market price of ilmenite concentrate these days is around $.06/pound.\n",
      "This prompted me to look up large potential terrestrial sources.\n",
      "\n",
      "On the moon, titanium occurs in basalts; \"high-Ti\" basalts (Apollo 11\n",
      "and 17) have 8-14% titanium dioxide (by weight).  This is nice, but...\n",
      "terrestrial continental flood basalts are also typically enriched in\n",
      "titanium.  They very often have 3% TiO2, frequently have 4%, and\n",
      "sometimes even 5% TiO2 (again, by weight).  These flood basalts are\n",
      "*enormous* -- millions of cubic kilometers, scattered all over the\n",
      "world (Siberia, Brazil, the NW United States, Ethiopia, etc.).  If\n",
      "even 1% of the basalts are 5% TiO2, this is trillions of tons of TiO2\n",
      "at concentrations only a factor of 2-3 less than in lunar high-Ti\n",
      "basalts.  It is difficult to see how the disadvantages of the moon\n",
      "could be overcome by such a small increase the concentration of the\n",
      "ore (never mind the richer, but less common, terrestrial ores being\n",
      "mined today).\n",
      "\n",
      "\tPaul F. Dietz\n",
      "\tdietz@cs.rochester.edu\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print new training data of 6th article - use dataset.data[index]\n",
    "print(newsgroups_train.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by using countvectorizer convert train data into numeric format considering only 1500 features\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "count_vect1 = CountVectorizer(max_features = 1500)\n",
    "vectors = count_vect1.fit_transform(newsgroups_train.data)\n",
    "vectors = vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use MultinomialNB(alpha=.01) for training - alpha is Laplace Smoothing \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "bnb = MultinomialNB(alpha=0.01)\n",
    "# fit\n",
    "bnb.fit(vectors,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by using countvectorizer convert test data into numeric format considering only 1500 features\n",
    "vectors_test = count_vect1.transform(newsgroups_test.data)\n",
    "vectors_test =vectors_test.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1102,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict target labels for testing set\n",
    "pred = bnb.predict(vectors_test)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1102,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test  = newsgroups_test.target\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9428312159709619"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find accuacy score on test set\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9337568058076225"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# used TfidfVectorizer insted of ContVectorizer and use multinomialNB\n",
    "#find test set accuracy\n",
    "\n",
    "count_vect = TfidfVectorizer(max_features = 1500)\n",
    "vectors_test = count_vect.fit_transform(newsgroups_train.data)\n",
    "vectors_test =vectors_test.toarray()\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB(alpha=0.01)\n",
    "# fit\n",
    "mnb.fit(vectors,y)\n",
    "\n",
    "#by using Tfidfvectorizer convert test data into numeric format considering only 1500 features\n",
    "vectors_test = count_vect.transform(newsgroups_test.data)\n",
    "vectors_test =vectors_test.toarray() \n",
    "\n",
    "#predict target labels for testing set\n",
    "pred = mnb.predict(vectors_test)\n",
    "\n",
    "#find accuacy score on test set\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try with avoiding stopwords and repeat the same\n",
    "\n",
    "\n",
    "#by using Countvectorizer convert train data into numeric format considering only 1500 features\n",
    "count_vect = CountVectorizer(max_features = 1500,stop_words='english')\n",
    "vectors = count_vect.fit_transform(newsgroups_train.data)\n",
    "vectors =vectors.toarray()\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB(alpha=0.01)\n",
    "# fit\n",
    "mnb.fit(vectors,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9437386569872959"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#by using Countvectorizer convert test data into numeric format considering only 1500 features\n",
    "vectors_test = count_vect.transform(newsgroups_test.data)\n",
    "vectors_test =vectors_test.toarray() \n",
    "\n",
    "#predict target labels for testing set\n",
    "pred = mnb.predict(vectors_test)\n",
    "\n",
    "#find accuacy score on test set\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(newsgroups_test.target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra - try grid search for count vectorizer, tfidf, different classifiers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra - try with stemming also"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
